<!DOCTYPE html>
<html>
<head>
    <title>3D Head-Coupled Perspective</title>
    <style>
        body { margin: 0; overflow: hidden; background: #000; font-family: sans-serif; }
        canvas { display: block; }
        #info {
            position: absolute; top: 10px; left: 10px; color: white;
            background: rgba(0,0,0,0.5); padding: 10px; pointer-events: none;
        }
        /* Hide the raw webcam feed in the corner */
        #video-container { position: absolute; bottom: 10px; right: 10px; width: 150px; border: 2px solid #555; }
        video { width: 100%; transform: scaleX(-1); } 
    </style>
</head>
<body>

    <div id="info">Move your head to change perspective</div>
    <div id="video-container"><video id="webcam" autoplay playsinline></video></div>

    <script type="importmap">
        {
            "imports": {
                "three": "https://unpkg.com/three@0.160.0/build/three.module.js",
                "three/addons/": "https://unpkg.com/three@0.160.0/examples/jsm/"
            }
        }
    </script>

    <script type="module">
        import * as THREE from 'three';
        import { FaceLandmarker, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";

        // --- CONFIGURATION ---
        const SCREEN_WIDTH_CM = 35; // Adjust to your actual monitor width
        const SCREEN_HEIGHT_CM = 20;
        let headPos = { x: 0, y: 0, z: 50 }; // Initial guess: 50cm away

        // --- THREE.JS SETUP ---
        const scene = new THREE.Scene();
        const renderer = new THREE.WebGLRenderer({ antialias: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        document.body.appendChild(renderer.domElement);

        // We use a custom projection, so starting params don't matter much
        const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);

        // Add a "Room" (The Grid Box)
        const roomSize = 40;
        const boxGeom = new THREE.BoxGeometry(roomSize, roomSize, roomSize);
        const boxMat = new THREE.MeshBasicMaterial({ 
            color: 0x00ff00, 
            wireframe: true, 
            side: THREE.BackSide 
        });
        const room = new THREE.Mesh(boxGeom, boxMat);
        room.position.z = -roomSize/2; // Push it back so the front is at Z=0
        scene.add(room);

        // Add a Placeholder Object (The "Shoe")
        const torusKnot = new THREE.Mesh(
            new THREE.TorusKnotGeometry(5, 1.5, 100, 16),
            new THREE.MeshNormalMaterial()
        );
        torusKnot.position.z = -15;
        scene.add(torusKnot);

        // --- MEDIAPIPE SETUP ---
        const video = document.getElementById("webcam");
        let faceLandmarker;

        async function setupFaceTracking() {
            const filesetResolver = await FilesetResolver.forVisionTasks("https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm");
            faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
                baseOptions: { modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task` },
                runningMode: "VIDEO"
            });
            
            const stream = await navigator.mediaDevices.getUserMedia({ video: true });
            video.srcObject = stream;
            video.addEventListener("loadeddata", predictWebcam);
        }

        async function predictWebcam() {
            const results = faceLandmarker.detectForVideo(video, performance.now());
            
            if (results.faceLandmarks && results.faceLandmarks.length > 0) {
                const face = results.faceLandmarks[0];
                // Landmark 1 is usually middle of the face
                // Map 0-1 range to screen dimensions
                headPos.x = (face[1].x - 0.5) * -SCREEN_WIDTH_CM; 
                headPos.y = (face[1].y - 0.5) * -SCREEN_HEIGHT_CM;
                // Z-depth estimation (simplified: based on distance between eyes)
                const eyeDist = Math.abs(face[33].x - face[263].x);
                headPos.z = 30 / (eyeDist + 0.1); 
            }
            requestAnimationFrame(predictWebcam);
        }

        // --- THE OFF-AXIS MATH ---
        function updateCamera() {
            const n = headPos.z; // Near plane is distance to screen
            const f = 1000;      // Far plane
            
            // Calculate frustum boundaries based on head position
            const l = -SCREEN_WIDTH_CM / 2 - headPos.x;
            const r =  SCREEN_WIDTH_CM / 2 - headPos.x;
            const b = -SCREEN_HEIGHT_CM / 2 - headPos.y;
            const t =  SCREEN_HEIGHT_CM / 2 - headPos.y;

            camera.projectionMatrix.makePerspective(l, r, t, b, n, f);
            camera.position.set(headPos.x, headPos.y, headPos.z);
            camera.lookAt(headPos.x, headPos.y, 0); // Always look straight at the screen plane
        }

        function animate() {
            updateCamera();
            torusKnot.rotation.y += 0.01;
            renderer.render(scene, camera);
            requestAnimationFrame(animate);
        }

        window.addEventListener('resize', () => {
            renderer.setSize(window.innerWidth, window.innerHeight);
        });

        setupFaceTracking();
        animate();
    </script>
</body>
</html>
